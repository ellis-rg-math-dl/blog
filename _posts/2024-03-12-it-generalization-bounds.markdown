---
layout: post
title:  "Information-theoretic generalization bounds for black-box learning algorithms"
date:   2024-03-12
categories: meetings
---

Hrayr Harutyunyan presented the information-theoretic perspective on generalization bounds based on the two papers:
NeurIPS21 ["Information-theoretic generalization bounds for black-box learning algorithms."](https://arxiv.org/pdf/2110.01584.pdf) and 
IEEE ITW22 ["Formal limitations of sample-wise information-theoretic generalization bounds.](https://arxiv.org/pdf/2205.06915.pdf).
We talked about the original ideas of bounding generalization gap for learning algorithms through mutual information of 
dataset and parameters of the learned model and how they were developed in the presented works. Further we discussed 
the limitations and possible workaround for further improvement of such bounds for the state-of-the-art machine learning models.

Presentation can be found [here](https://drive.google.com/file/d/1tGEYsYyuZGVDoSmVSCsfRt9BKSDhInib/view?usp=sharing).